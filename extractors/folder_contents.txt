network_id: 8453

blocks:
  min_block: "7.5M"
  requests_per_second: 25
  block_increment: 500
  chunk_size: 80

eth_calls:
  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultCollateral"
    inputs:
      - [1, "0xC74eA762cF06c9151cE074E6a569a5945b6302E7"]
      - [1, "0x729Ef31D86d31440ecBF49f27F7cD7c16c6616d2"]
    min_block: "7.5M"
    requests_per_second: 25
    block_increment: 500
    chunk_size: 80

  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultDebt"
    inputs:
      - [1, "0xC74eA762cF06c9151cE074E6a569a5945b6302E7"]
      - [1, "0x729Ef31D86d31440ecBF49f27F7cD7c16c6616d2"]
    min_block: "7.5M"
    requests_per_second: 25
    block_increment: 500
    chunk_size: 80
network_id: 84532

blocks:
  min_block: "8M"
  requests_per_second: 25
  block_increment: 500
  chunk_size: 80

eth_calls:
  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultCollateral"
    inputs:
      - [1, "0x8069c44244e72443722cfb22DcE5492cba239d39"]
    min_block: "8M"
    requests_per_second: 25
    block_increment: 500
    chunk_size: 80

  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultDebt"
    inputs:
      - [1, "0x8069c44244e72443722cfb22DcE5492cba239d39"]
    min_block: "8M"
    requests_per_second: 25
    block_increment: 500
    chunk_size: 80
network_id: 421614

blocks:
  min_block: "41M"
  requests_per_second: 25
  block_increment: 4000
  chunk_size: 80

eth_calls:
  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultCollateral"
    inputs:
      - [1, "0x980B62Da83eFf3D4576C647993b0c1D7faf17c73"]
      - [1, "0x7b356eEdABc1035834cd1f714658627fcb4820E3"]
      - [1, "0x75faf114eafb1BDbe2F0316DF893fd58CE46AA4d"]
      - [1, "0xda7b438d762110083602AbC497b1Ec8Bc6605eC9"]
      - [1, "0x54664815B709252dDC99dB3CB91e2d584717DbfC"]
      - [1, "0x4159018C381e5AEB9A95AB27c26726fBc4671f08"]
      - [1, "0x7FcAD85b378D9a13733dD5c715ef318F45cd7699"]
    min_block: "41M"
    requests_per_second: 25
    block_increment: 4000
    chunk_size: 80

  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultDebt"
    inputs:
      - [1, "0x980B62Da83eFf3D4576C647993b0c1D7faf17c73"]
      - [1, "0x7b356eEdABc1035834cd1f714658627fcb4820E3"]
      - [1, "0x75faf114eafb1BDbe2F0316DF893fd58CE46AA4d"]
      - [1, "0xda7b438d762110083602AbC497b1Ec8Bc6605eC9"]
      - [1, "0x54664815B709252dDC99dB3CB91e2d584717DbfC"]
      - [1, "0x4159018C381e5AEB9A95AB27c26726fBc4671f08"]
      - [1, "0x7FcAD85b378D9a13733dD5c715ef318F45cd7699"]
    min_block: "41M"
    requests_per_second: 25
    block_increment: 4000
    chunk_size: 80
network_id: 42161

blocks:
  min_block: "232500000"
  requests_per_second: 25
  block_increment: 4000
  chunk_size: 80

eth_calls:
  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultCollateral"
    inputs:
      - [1, "0x82aF49447D8a07e3bd95BD0d56f35241523fBab1"]
      - [1, "0x912CE59144191C1204E64559FE8253a0e49E6548"]
      - [1, "0xaf88d065e77c8cC2239327C5EDb3A432268e5831"]
      - [1, "0x5d3a1Ff2b6BAb83b63cd9AD0787074081a52ef34"]
      - [1, "0x35751007a407ca6FEFfE80b3cB397736D2cf4dbe"]
      - [1, "0x5979D7b546E38E414F7E9822514be443A4800529"]
      - [1, "0x211Cc4DD073734dA055fbF44a2b4667d5E5fE5d2"]
    min_block: "218M"
    requests_per_second: 25
    block_increment: 4000
    chunk_size: 80

  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultDebt"
    inputs:
      - [1, "0x82aF49447D8a07e3bd95BD0d56f35241523fBab1"]
      - [1, "0x912CE59144191C1204E64559FE8253a0e49E6548"]
      - [1, "0xaf88d065e77c8cC2239327C5EDb3A432268e5831"]
      - [1, "0x5d3a1Ff2b6BAb83b63cd9AD0787074081a52ef34"]
      - [1, "0x35751007a407ca6FEFfE80b3cB397736D2cf4dbe"]
      - [1, "0x5979D7b546E38E414F7E9822514be443A4800529"]
      - [1, "0x211Cc4DD073734dA055fbF44a2b4667d5E5fE5d2"]
    min_block: "218M"
    requests_per_second: 25
    block_increment: 4000
    chunk_size: 80
network_id: 1

blocks:
  min_block: "20000000"
  requests_per_second: 25
  block_increment: 150
  chunk_size: 50

eth_calls:
  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultCollateral"
    inputs:
      - [1, "0xC011a73ee8576Fb46F5E1c5751cA3B9Fe0af2a6F"]
    min_block: "20000000"
    requests_per_second: 25
    block_increment: 150
    chunk_size: 50

  - contract_name: "CoreProxy"
    package_name: "system"
    function_name: "getVaultDebt"
    inputs:
      - [1, "0xC011a73ee8576Fb46F5E1c5751cA3B9Fe0af2a6F"]
    min_block: "20000000"
    requests_per_second: 25
    block_increment: 150
    chunk_size: 50
import os
import yaml
import argparse
from dotenv import load_dotenv
from src.extract import extract_blocks, extract_data

# load environment variables
load_dotenv()

# parse command-line arguments
parser = argparse.ArgumentParser(description="Extract data from Ethereum nodes.")
parser.add_argument("config", help="Path to the YAML configuration file")
parser.add_argument("--name", help="Name of the configuration to use (optional)")
args = parser.parse_args()

# load configurations from YAML file
with open(args.config, "r") as f:
    config = yaml.safe_load(f)

network_id = config.get("network_id")
block_config = config.get("blocks")
eth_call_configs = config.get("eth_calls", [])

# determine the flow based on the --name argument
if args.name:
    if args.name == "blocks":
        # run blocks only
        extract_blocks(network_id=network_id, **block_config)
    else:
        # run the specified eth_call only
        eth_call_config = next(
            (ec for ec in eth_call_configs if ec["function_name"] == args.name), None
        )
        if eth_call_config:
            extract_data(network_id=network_id, **eth_call_config)
        else:
            print(f"No configuration found with name {args.name}")
else:
    # run everything
    exceptions = []
    try:
        extract_blocks(network_id=network_id, **block_config)
    except Exception as e:
        exceptions.append(e)
        print(f"Error extracting blocks: {e}")

    for eth_call_config in eth_call_configs:
        try:
            extract_data(network_id=network_id, **eth_call_config)
        except Exception as e:
            exceptions.append(e)
            print(f"Error extracting eth_call {eth_call_config.get('name')}: {e}")
            continue

    # if there are any exceptions, raise the first one
    if len(exceptions) > 0:
        raise Exception(exceptions[0])
from pathlib import Path
import re
from web3._utils.abi import get_abi_output_types, get_abi_input_types
from eth_abi import decode
from eth_utils import decode_hex
import polars as pl
import duckdb


def fix_labels(labels):
    return [
        f"value_{i + 1}" if label == "" else label for i, label in enumerate(labels)
    ]


def ensure_directory_exists(file_path):
    # Use pathlib to handle path operations
    directory = Path(file_path).parent
    # Create the directory if it does not exist
    directory.mkdir(parents=True, exist_ok=True)


def decode_data(contract, function_name, result, is_input=True):
    # get the function abi
    func_abi = contract.get_function_by_name(function_name).abi
    if is_input:
        types = get_abi_input_types(func_abi)
    else:
        types = get_abi_output_types(func_abi)

    # decode the result
    return decode(types, result)


def decode_call(contract, function_name, call):
    if call is None or call == "0x":
        return None
    else:
        decoded = [
            str(i)
            for i in decode_data(
                contract, function_name, decode_hex(f"0x{call[10:]}"), is_input=True
            )
        ]
        return decoded


def decode_output(contract, function_name, call):
    if call is None or call == "0x":
        return None
    else:
        return [
            str(i)
            for i in decode_data(
                contract, function_name, decode_hex(call), is_input=False
            )
        ]


def camel_to_snake(name):
    name = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", name).lower()
    return name


def get_labels(contract, function_name):
    functions = contract.find_functions_by_name(function_name)
    if len(functions) > 0:
        function = functions[0]
    else:
        raise ValueError(f"Function {function_name} not found in contract")

    input_names = [camel_to_snake(i["name"]) for i in function.abi["inputs"]]
    output_names = [camel_to_snake(i["name"]) for i in function.abi["outputs"]]

    return input_names, output_names


def clean_data(chain_name, contract, function_name, write=True):
    input_labels, output_labels = get_labels(contract, function_name)

    # fix labels
    input_labels = fix_labels(input_labels)
    output_labels = fix_labels(output_labels)

    # read and dedupe the data
    df = duckdb.sql(
        f"""
        SELECT DISTINCT *
        FROM '../parquet-data/raw/{chain_name}/{function_name}/*.parquet'
        WHERE
            call_data IS NOT NULL
            AND output_data IS NOT NULL
            AND output_data != '0x'
        ORDER BY block_number
    """
    ).pl()

    # Decode call_data and output_data, then convert lists to multiple columns
    df = df.with_columns(
        [
            pl.col("call_data")
            .map_elements(lambda call: decode_call(contract, function_name, call))
            .alias("decoded_call_data"),
            pl.col("output_data")
            .map_elements(lambda output: decode_output(contract, function_name, output))
            .alias("decoded_output_data"),
            pl.col("block_number").cast(pl.Int64),
        ]
    )

    # Expand decoded_call_data into separate columns based on input_labels
    for i, label in enumerate(input_labels):
        df = df.with_columns(
            pl.col("decoded_call_data").map_elements(lambda x: x[i]).alias(label)
        )

    # Expand outputs into separate columns based on output_labels
    for i, label in enumerate(output_labels):
        df = df.with_columns(
            pl.col("decoded_output_data").map_elements(lambda x: x[i]).alias(label)
        )

    # Remove the original list columns if no longer needed
    df = df.drop(
        ["call_data", "output_data", "decoded_call_data", "decoded_output_data"]
    )

    # write the data
    if write:
        file_path = f"../parquet-data/clean/{chain_name}/{function_name}.parquet"

        ensure_directory_exists(file_path)
        # write the data
        duckdb.sql(
            f"""
            COPY df to '{file_path}' (FORMAT PARQUET, OVERWRITE_OR_IGNORE)
        """
        )

    return df


def clean_blocks(chain_name, write=True):
    # read the data
    df = duckdb.sql(
        f"""
        SELECT DISTINCT
            CAST(timestamp as BIGINT) as timestamp,
            CAST(block_number as BIGINT) as block_number
        FROM '/parquet-data/raw/{chain_name}/blocks/*.parquet'
        ORDER BY block_number
    """
    )

    # write the data
    if write:
        file_path = f"/parquet-data/clean/{chain_name}/blocks.parquet"

        ensure_directory_exists(file_path)

        # write the data
        duckdb.sql(
            f"""
            COPY df to '{file_path}' (FORMAT PARQUET, OVERWRITE_OR_IGNORE)
        """
        )

    return df
import os
from dotenv import load_dotenv

load_dotenv()


CHAIN_CONFIGS = {
    1: {
        "name": "eth_mainnet",
        "rpc": os.getenv("NETWORK_1_RPC"),
        "network_id": 1,
        "cannon_config": {
            "package": "synthetix-omnibus",
            "version": "latest",
            "preset": "main"
        }
    },
    10: {
        "name": "optimism_mainnet",
        "rpc": os.getenv("NETWORK_10_RPC"),
        "network_id": 10,
    },
    8453: {
        "name": "base_mainnet",
        "rpc": os.getenv("NETWORK_8453_RPC"),
        "network_id": 8453,
    },
    84532: {
        "name": "base_sepolia",
        "rpc": os.getenv("NETWORK_84532_RPC"),
        "network_id": 84532,
    },
    42161: {
        "name": "arbitrum_mainnet",
        "rpc": os.getenv("NETWORK_42161_RPC"),
        "network_id": 42161,
    },
    421614: {
        "name": "arbitrum_sepolia",
        "rpc": os.getenv("NETWORK_421614_RPC"),
        "network_id": 421614,
    },
}
import cryo
from synthetix import Synthetix
from .constants import CHAIN_CONFIGS
from .clean import clean_data, clean_blocks


def get_synthetix(chain_config):
    if "cannon_config" in chain_config:
        return Synthetix(
            provider_rpc=chain_config["rpc"],
            network_id=chain_config["network_id"],
            cannon_config=chain_config["cannon_config"],
        )
    else:
        return Synthetix(
            provider_rpc=chain_config["rpc"],
            network_id=chain_config["network_id"],
        )


# generalize a function
def extract_data(
    network_id,
    contract_name,
    package_name,
    function_name,
    inputs,
    clean=True,
    min_block=0,
    requests_per_second=25,
    block_increment=500,
    chunk_size=1000,
):
    if network_id not in CHAIN_CONFIGS:
        raise ValueError(f"Network id {network_id} not supported")

    # get synthetix
    chain_config = CHAIN_CONFIGS[network_id]
    snx = get_synthetix(chain_config)
    output_dir = f"/parquet-data/raw/{chain_config['name']}/{function_name}"

    # encode the call data
    contract = snx.contracts[package_name][contract_name]["contract"]
    calls = [
        contract.encodeABI(fn_name=function_name, args=this_input)
        for this_input in inputs
    ]

    cryo.freeze(
        "eth_calls",
        contract=[contract.address],
        function=calls,
        blocks=[f"{min_block}:latest:{block_increment}"],
        rpc=snx.provider_rpc,
        requests_per_second=requests_per_second,
        chunk_size=chunk_size,
        output_dir=output_dir,
        hex=True,
        exclude_failed=True,
    )

    if clean:
        df_clean = clean_data(chain_config["name"], contract, function_name)


def extract_blocks(
    network_id,
    clean=True,
    min_block=0,
    requests_per_second=25,
    block_increment=500,
    chunk_size=1000,
):
    if network_id not in CHAIN_CONFIGS:
        raise ValueError(f"Network id {network_id} not supported")

    # get synthetix
    chain_config = CHAIN_CONFIGS[network_id]
    snx = get_synthetix(chain_config)

    # try reading and looking for latest block
    output_dir = f"/parquet-data/raw/{chain_config['name']}/blocks"

    cryo.freeze(
        "blocks",
        blocks=[f"{min_block}:latest:{block_increment}"],
        rpc=snx.provider_rpc,
        requests_per_second=requests_per_second,
        chunk_size=chunk_size,
        output_dir=output_dir,
        hex=True,
        exclude_failed=True,
    )

    if clean:
        df_clean = clean_blocks(chain_config["name"])
